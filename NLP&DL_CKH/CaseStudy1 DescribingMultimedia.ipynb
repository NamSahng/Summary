{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## case 1. Learning to Describe Multimedia\n",
    "\n",
    "- 지금까지 Sentence Representation에서부터 Machine Translation 까지의 딥러닝을 이용한 자연어처리를 활용한 것을 보았다.\n",
    "\n",
    "\n",
    "- Machine Translation을 보면,\n",
    "    - Input: a sentence written in a source language\n",
    "    - Output: a corresponding sentence in a target language\n",
    "    - 의 형태에서 Input이 굳이 natural language sentence가 아닌 다른 멀티미디어 데이터여도 이를 설명하는 문장을 output이 나올 것 같다는 질문에 case1은 시작된다.\n",
    "\n",
    "<img  src=\"./image/img_c1-1.PNG\" width=\"70%\"> \n",
    "\n",
    "- 어차피 Machine Translation의 모델이 보는 input은 Continuous vector of sentece니까(sentence가 아닌)\n",
    "    - 따라서, input으로 주어진 것이 language가 아닌 image, speech, video일 때도 이를 continuous vector로 바꾸면, 뒤의 NN 구조를 그대로 사용할 수 있다.\n",
    "    \n",
    "### a. Image Caption Generation [Xu et al., 2015]\n",
    "\n",
    "<img  src=\"./image/img_c1-2.PNG\" width=\"70%\"> \n",
    "\n",
    "- Input: an image\n",
    "- Output: an image caption\n",
    "- Network Architecture:\n",
    "    - Encoder: deep convolution network \n",
    "        - Encoding part의 Attention에서 image representation을 찾는 애로 바꾸어 주면된다.\n",
    "        - 그리고 image recognition을 찾는데 이미 많은 CNN 모델이 잘되있는 모델이 너무 많다. (당시 VGG, Alexnet)\n",
    "    - Decoder: recurrent language model with the attention mechanism.\n",
    "- Data: image-caption pairs\n",
    "    - 이 데이터도 Flicker, MS의 COCO 등 많다\n",
    "    \n",
    "- 그리고 Attention의 Weight을 보면 visualize할 수 있고 이를 하면 다음과 같이 나왔다.\n",
    "    - Attention에서 이미지의 높은 weight을 받은 것이, 밑줄 친 문장\n",
    "<img  src=\"./image/img_c1-3.PNG\" width=\"90%\"> \n",
    "\n",
    "- 잘은 모르겠지만 약간 이것도 Cherry Picking인 느낌으로 말하긴 하시지만, Network이 우리가 원하는 것을 함을 알 수 있다.\n",
    "\n",
    "### b. Video Description Generation [Li Yao et al., 2015]\n",
    "\n",
    "- 그리고 위의 기반과 때 마침 Youtube에서 나온 Video Clip과 Description의 데이터로 Video Description Generation을 시도해보았다.\n",
    "\n",
    "\n",
    "- Input: a short video clip - a sequence of video frames.\n",
    "- Output: a corresponding description\n",
    "- Network Architecture\n",
    "    - Encoder: a deep 2+3D convolutional network (조금 복잡)\n",
    "        - 1. A 2-D convolutional network for each frame (각 프레임은 2D로)\n",
    "        - 2. A 3-D convolutional network for the entire clip (Video 전체는 3D로)\n",
    "    - Decoder: recurrent language modelwith the attention mechanism\n",
    "\n",
    "- 그러나 역시 Video clip Description은 잘 안됐다.\n",
    "    - Video Clip의 몇개 frame과 Description의 문장에서의 Attention weight의 변화\n",
    "<img  src=\"./image/img_c1-4.PNG\" width=\"90%\"> \n",
    "\n",
    "### c. Speech Recognition [Chorowski et al., 2015]\n",
    "\n",
    "- Input: Speech (스펙트럼)\n",
    "- Output: transcription\n",
    "- Network Architecture\n",
    "    - Encoder: convolution+recurrent acoustic network\n",
    "    - Decoder: conditional recurrent language model + attention mechanism\n",
    "\n",
    "- 그러나 speech의 길이가 워낙 긴 문제를 여러가지 방식을 시도해서 당시에는 적당히(?) 잘 된 결과를 보여줌.\n",
    "\n",
    "<img  src=\"./image/img_c1-5.PNG\" width=\"80%\"> \n",
    "\n",
    "- Target쪽에서는 각 phoneme(음소) sequence를 뽑아내게 중간에 각 phoneme이 스펙트럼의 어디에 alignment 없이 training 시켜 visualize해본것 \n",
    "    - \"Michael Colored the bedroom wall with crayons.\" 이라는 val set에서\n",
    "\n",
    "<img  src=\"./image/img_c1-6.PNG\" width=\"80%\"> \n",
    "\n",
    "- 그리고 Attention weight을 계속 check하면 자연스럽게 움직이는 것을 볼 수 있다.\n",
    "- 당시에는 그냥 concept을 체크하는 부분이긴한데\n",
    "- 일종의 Speech Recognition SOTA 시스템은 이와 유사한 시스템을 사용하고 있다.\n",
    "    - 물론 훨씬 크긴하지만.\n",
    "\n",
    "- 이를 통해, Attention, CNN, RNN을 사용해 복잡하고 다양한 문제에서 풀 수 있겠다 한다.\n",
    "\n",
    "\n",
    "### Since 2015…\n",
    "\n",
    "- The attention (alignment) mechanism has become a work horse behind various AI models/applications including\n",
    "    - Neural Turing machines (differentiable neural computer) [Graves et al., 2015&2016], memory networks [Weston et al., 2015; Sukhbaatar et al., 2016], dynamic neural Turing machines [Gulcehre et al., 2017; Miller et al., 2017], …\n",
    "    - Reinforcement learning: attentive history selection [Tian et al., 2016], neural episodic control [Pritzel et al., 2017]\n",
    "    - Generative models: DRAW [Gregor et al., 2016], Image Transformer [Parmar et al., 2018], …\n",
    "    \n",
    "- 이렇게 Attention 메커니즘들이 다양하게 많이 쓰이는 것을 보았고, 여기서 가장 중요한 포인트는 Continuous Vector space에 Encoding (projection) 하는 것.\n",
    "\n",
    "- 그래도 이제 시작이다. 박테리아보다 못하다. 이제 눈좀뜨고 살짝 듣고 그런 수준이다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: \n",
    "\n",
    "- https://www.edwith.org/deepnlp 조경현교수님, 딥러닝을 이용한 자연어 처리 강의 및 강의 자료 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image:   https://arxiv.org/abs/1502.03044\n",
    "- video:   https://arxiv.org/abs/1502.08029\n",
    "- speech:  https://arxiv.org/abs/1506.07503"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
