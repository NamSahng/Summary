{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.  Introduction\n",
    "\n",
    "Generalization: 사용되지 않았던 새로운 예시들을 올바르게 분류하는 능력\n",
    "\n",
    "### 1.1 다항식 곡선(Polynomial Curve)\n",
    "\n",
    "우리의 목표는 training set을 사용해 새로운 입력값 $\\hat{x}$이 주어졌을 때, Target Variable $\\hat{t}$를 예측하는 것이다. 우리가 앞으로 살펴볼 것처럼 기저에 있는 함수 $sin(2{\\pi}x)$를 찾아내는 것이 예측과정에 암시적으로 포함된다. 이는 한정된 데이터 집합으로부터 Generalization을 시행하는 과정이 필요하기 때무에 본질적으로 어려운 문제이다. 게다가 Observed data들은 Noise로 인해 변질되어 있어서 각각의 주어진 $\\hat{x}$에 대해 어떤 값이 적합한 $\\hat{t}$인지 불확실하다.\n",
    "\n",
    "확률론(1.2)에서는 이러한 불확실성을 정확하고 정량적으로 표현하는데 도움을 주고 \n",
    "의사결정이론(1.5)에서는 특정 기준에 따라 최적의 예측을 하는 데 확률적인 표현을 활용할 수 있게 해준다.\n",
    "\n",
    "- $sin(2{\\pi}x)$에서 random noise 섞은 데이터가 10 개일 때\n",
    "\n",
    "9차에서 오차 0 $\\to$ 당연, 해당 다항식은 $w_0$ ~ $w_9$까지 10차의 자유도를 갖고 있고 데이터도 10개이므로 but Overfitting\n",
    "\n",
    "데이터가 많으면, 복잡한(유연한) 모델을 활용한 피팅이 가능. 모델의 복잡도를 측정하는데 매개변수의 숫자만을 사용하는 것이 아닌 더 적합한 방법이 존재한다. (3장 선형회귀모델, Linear Regression Model)\n",
    "\n",
    "지금의 예시에서 사용한 최소제곱법(least squares approximation)은 최대 가능도(Maximum Liklihood 1.2.5)의 특별한 사례이다. Overfitting 문제는 Maximum Liklihood 방법의 성질 중 하나로써 이해가 가능하다. Bayesian 방법론을 채택하면 과적합 문제를 피할 수가 있다. Bayesian 관점에서는 데이터 포인트의 숫자보다 매개변수의 숫자가 훨씬 더 많은 모델을 사용해도 문제가 없다는 것을 앞으로 볼 것이다. Bayesian 모델에서는 데이터 집합의 크기에 따라서 적합한 매개변수의 수가 자동으로 정해진다.\n",
    "\n",
    "비교적 복잡하고 유연한 모델을 제한적인 숫자의 데이터 집합을 활용해 Fitting 할 때 자주 사용되는 기법이 Regularization 이다. Error Function에서 penalty항을 추가하는 것이다.\n",
    "\n",
    "$\\tilde{E}(w) = {1\\over 2}{\\Sigma \\{ y(x_{n},\\mathbf{w}) - t_{n} \\} ^{2} + {\\lambda \\over 2}  {\\parallel}{\\mathbf{w}}{\\parallel}^2  }$\n",
    "\n",
    "$ {\\parallel}{\\mathbf{w}}{\\parallel}^2 \\equiv {\\mathbf{w}}^{T}{\\mathbf{w}} = w^{2}_{0} + w^{2}_{1} + w^{2}_{2} + ... + w^{2}_{M}   $이고 계수  ${\\lambda}$ 가 정규화항의 제곱합 Error Function에 대한 상대적인 중요도를 결정짓는다. ${w_{0}}$을 포함하면 Target Variable의 원점을 무엇으로 선택택하느냐에 대해 결과가 종속되므로, 종종 ${w_{0}}$는 정규화항에서 제외한다. ${w_{0}}$만 따로 빼내어 별도의 정규화 계수와 함께 다른 항을 만들어 포함하기도 한다. (5장 5.1)\n",
    "\n",
    "$\\tilde{E}(w)$ 의 최솟값을 찾는 문제 역시 닫힌 형식이기 때문에 앞에서 미분을 통해 유일해를 찾아낼 수 있다. 통계학에서는 계수의 크기를 수축시키는 방법을 사용하여 수축법(Shrinkage Method)라고 하고, Quadratic(이차 형식) 정규화는 Ridge Regression이라고 부룬다. Neural Network의 맥락에서는 이를 Weight Decay(가중치 감쇠)라 한다.\n",
    "\n",
    "위에서 M = 9 일 때, Overfitting이 일어난 식에 $ln\\lambda = -18$ 을 적용하면 Overfitting을 피할 수 있다. 그러나 람다가 너무 크면 좋지 않다. 모델의 복잡도와 람다의 관계는 1.3 에서 자세히 본다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 확률론\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
